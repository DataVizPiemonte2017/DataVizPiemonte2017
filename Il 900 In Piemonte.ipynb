{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://breakingcode.wordpress.com/2010/06/29/google-search-python/\n",
    "# https://github.com/MarioVilas/google\n",
    "!pip install git+git://github.com/MarioVilas/google.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import overpass as op\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import codecs\n",
    "import pygeoj\n",
    "import locale\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "api = op.API(timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of valid instance\n",
    "instances =[\"determinator for date of periodic occurrence\",\"human\"]  \n",
    "\n",
    "#dizionario da 0 a 1997, per ogni key inserisci l'id'del periodo\n",
    "setRemoveKey = [\"strada\",\"antica\",\"via\",\n",
    "         \"corso\", \"piazza\", \"viale\", \"borgata\", \"provinciale\",\"casale\",\"tetto\", \"vicolo\", \"località\",\n",
    "         \"colle\",\"ponte\", \"tetti\", \"frazione\", \"sentiero\", \n",
    "         \"di\",\"del\",\"dei\", \"dello\", \"della\",\"di\", \n",
    "         \"rocca\", \"privata\", \"viale\",\"della\", \"sentiero\", \"pista\", \"rondò\",\"per\",\"circonvallazione\",\"comunale\",\"passeggiata\"]\n",
    "timeSlot={}\n",
    "for i in range(2017):\n",
    "    if i >= 0 and i <=999: x=1\n",
    "    elif i >=1000\tand i<=1249: x=2\n",
    "    elif i >=1250\tand i<=1491: x=3\n",
    "    elif i >=1492\tand i<=1599: x=4\n",
    "    elif i >=1600\tand i<=1699: x=5\n",
    "    elif i >=1700\tand i<=1788: x=6\n",
    "    elif i >=1789\tand i<=1830: x=7\n",
    "    elif i >=1831\tand i<=1860: x=8\n",
    "    elif i >=1861\tand i<=1900: x=9\n",
    "    elif i >=1901\tand i<=1920: x=10\n",
    "    elif i >=1920\tand i<=1925: x=11\n",
    "    elif i >=1925\tand i<=1940: x=12\n",
    "    elif i >=1936\tand i<=1940: x=13\n",
    "    elif i >=1941\tand i<=1942: x=14\n",
    "    elif i >=1943\tand i<=1946: x=15\n",
    "    elif i >=1947\tand i<=1948: x=16\n",
    "    elif i >=1949\tand i<=1954: x=17\n",
    "    elif i >=1955\tand i<=1965: x=18\n",
    "    elif i >=1966\tand i<=1978: x=19\n",
    "    elif i >=1978\tand i<=1993: x=20\n",
    "    elif i >=1993\tand i<=1995: x=21\n",
    "    elif i >=1995\tand i<=1997: x=22\n",
    "    elif i>=1998: x=23    \n",
    "    timeSlot[i]=x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#File manager\n",
    "def save_file(cityKey,cityJson):\n",
    "    with open('%s.json' % cityKey, 'w') as f: \n",
    "        json.dump(cityJson, f)\n",
    "        \n",
    "def open_file(fileName):\n",
    "    with codecs.open(fileName, encoding='utf-8') as json_data:\n",
    "        cityJson = json.load(json_data)\n",
    "        return cityJson\n",
    "    \n",
    "def open_fileGeoJson(filename):\n",
    "    with open(filename)  as json_data:\n",
    "        cityJson = json.load(json_data)\n",
    "        return cityJson    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge data with Wikipedia Links and keep only these streets\n",
    "def addKeyFromWiki (dicNameKey,wikiList):\n",
    "    content = []\n",
    "    for via in wikiList:\n",
    "            link =  wikiList[via]\n",
    "            listId = dicNameKey[via.encode(\"UTF-8\")]\n",
    "            #One entry per Idx (1 street : n Idx)\n",
    "            for idx in listId: \n",
    "                dicBase = {'idx':\"\",'via':\"\",'wu':\"\"} \n",
    "                dicBase['idx']= idx\n",
    "                dicBase['via']= via \n",
    "                dicBase['wu']= link \n",
    "                content.append(dicBase)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add Scrapy info: merge scrapy file: link:dati al file che hai usato per fare query su wikipedia: nome via - via token\n",
    "#rinominare il campo name di scrapy in wn, aggiungere flag su data se human, capire eventi\n",
    "\n",
    "def addScrapy(content, scrapyFile):\n",
    "    local= locale.setlocale(locale.LC_TIME,(\"en\",\"us\")) #for dates\n",
    "    #creo dizionario di scarpy con key url\n",
    "    count900 =0\n",
    "    dicScrapy = {}\n",
    "    for i in scrapyFile:\n",
    "        urlWiki = i[\"wiki_url\"]\n",
    "        dicScrapy[urlWiki] = i\n",
    "    # ci sono dei link per cui non c'è scrapy print dicScrapy[\"https://it.wikipedia.org/wiki/Giorgio_Ravaz\"] \n",
    "    #incrocio i dati con il link key da dizionario\n",
    "    \n",
    "    for idx in content:\n",
    "        link = idx[\"wu\"]\n",
    "        del idx['wu']  #lo cancello per non avere il duplicato  \n",
    "        if link in dicScrapy:\n",
    "            scrapyData = dicScrapy[link]\n",
    "            if \"wiki_url\" in scrapyData:\n",
    "                scrapyData[\"wu\"] = scrapyData.pop(\"wiki_url\")#rinominato campo\n",
    "            if \"name\" in scrapyData:\n",
    "                scrapyData[\"wn\"] = scrapyData.pop(\"name\")#rinominato campo\n",
    "            if \"wd_url\" in scrapyData:\n",
    "                scrapyData[\"wd\"] = scrapyData.pop(\"wd_url\")#rinomino wiki data\n",
    "            if \"img_url\" in scrapyData:   \n",
    "                linkImg = scrapyData[\"img_url\"][0]\n",
    "                scrapyData.pop(\"img_url\")\n",
    "                scrapyData[\"img\"] = linkImg\n",
    "            \n",
    "            #check su data:\n",
    "            if scrapyData[\"instance_type\"] in instances:\n",
    "                #if human or event\n",
    "                if \"date_of_death\" in scrapyData:\n",
    "                    yearM = re.match('.*([1-3][0-9]{3})', scrapyData[\"date_of_death\"])\n",
    "                    if yearM:\n",
    "                        year= yearM.group(1)\n",
    "                        #print year\n",
    "                        if year>=1900: count900+= 1\n",
    "                        scrapyData[\"yd\"]=str(year)\n",
    "                if \"date_of_birth\" in scrapyData:\n",
    "                    yearM = re.match('.*([1-3][0-9]{3})', scrapyData[\"date_of_birth\"])\n",
    "                    if yearM:\n",
    "                        year= yearM.group(1)\n",
    "                        scrapyData[\"yb\"]=str(year)  \n",
    "            idx.update(scrapyData) #dovrebbe mergiare i due dizionari \n",
    "            #del idx['wu']  #lo cancello per non avere il duplicato  \n",
    "    print \"COUNT \"+str(count900)           \n",
    "    return content              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep GeoJson data just for idx with some scrapy data\n",
    "def dicReducedGeo(gsonFile, content):\n",
    "    newGeofile = pygeoj.new()\n",
    "    newGeofile.define_crs(type=\"link\", link=\"http://spatialreference.org/ref/epsg/26912/esriwkt/\", link_type=\"esriwkt\")\n",
    "            \n",
    "    for properties in content:  \n",
    "        idx= str(properties['idx'])\n",
    "        for feature in gsonFile:\n",
    "            idxF = feature.properties[\"id\"]\n",
    "            if idxF == idx:\n",
    "                newGeofile.add_feature(properties=properties, geometry=feature.geometry)\n",
    "    \n",
    "    return newGeofile \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python script to execute from Console\n",
    "from google import search\n",
    "import json\n",
    "import sys\n",
    "\n",
    "for arg in sys.argv: 1\n",
    "print arg\n",
    "\n",
    "city = arg\n",
    "links = {}\n",
    "filename = \"%s.json\" % city\n",
    "filenameSaved = \"W_%s.json\" % city\n",
    "#filename = \"TO05.json\"\n",
    "\n",
    "print filename\n",
    "\n",
    "with open(filename) as json_data:\n",
    "    wikiTest_short = json.load(json_data)\n",
    "    for i in wikiTest_short:\n",
    "        value = wikiTest_short[i].encode('utf-8')\n",
    "        #check = \"Check %s and %s\" %(i,wikiTest_short[i])\n",
    "        #print check\n",
    "        for url in search(value, tld='it', lang='it', stop=5):\n",
    "            if url.startswith('https://it.wikipedia.org'):\n",
    "                links [i] = url\n",
    "                print url\n",
    "                break\n",
    "    #salvami dizionario con chiave la strada e valore il link wikipedia se lo hai trovato\n",
    "    with open(filenameSaved, 'w') as fp:\n",
    "        json.dump(links, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep GeoJson data just for idx with some scrapy data\n",
    "def dicReducedGeoOnlyInstance(gsonFile, content):\n",
    "    newGeofile = pygeoj.new()\n",
    "    newGeofile.define_crs(type=\"link\", link=\"http://spatialreference.org/ref/epsg/26912/esriwkt/\", link_type=\"esriwkt\")\n",
    "            \n",
    "    for properties in content:  \n",
    "        idx= str(properties['idx'])\n",
    "        #print properties\n",
    "        if \"instance_type\" in properties.keys(): #and properties[\"instance_type\"] in instances:\n",
    "            if properties[\"instance_type\"] in instances:\n",
    "                for feature in gsonFile:\n",
    "                    idxF = feature.properties[\"id\"]\n",
    "                    if idxF == idx:\n",
    "                        newGeofile.add_feature(properties=properties, geometry=feature.geometry)\n",
    "    \n",
    "    return newGeofile \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1_ partendo dal geoJson crearsi dic con id e via\n",
    "cityKey = \"Cuneo\"  \n",
    "\n",
    "geoJson = pygeoj.load(filepath=\"%s.geojson\" %cityKey) \n",
    "id_name ={}\n",
    "name_id ={}\n",
    "for feature in geoJson: #distinti per id\n",
    "    if feature.properties[\"name\"]:\n",
    "        name = feature.properties[\"name\"].encode('utf-8')\n",
    "       \n",
    "        idx = feature.properties[\"id\"]\n",
    "        #dict key and name\n",
    "        id_name[feature.properties[\"id\"]]= name\n",
    "         \n",
    "        if name in name_id:    \n",
    "            lista = name_id[name]\n",
    "            lista.append(feature.properties[\"id\"])\n",
    "        else:\n",
    "            idList = []\n",
    "            idList.append(feature.properties[\"id\"])\n",
    "            name_id[name]=idList\n",
    "            \n",
    "#print id_name\n",
    "print len(name_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean and token street name in name_idt\n",
    "wikiAskDic = list()\n",
    "askGoogleDic = {}\n",
    "for street in name_id:\n",
    "        lowerStreet=street.lower()\n",
    "        streetToken = lowerStreet.split()\n",
    "        streetTokenL = list(streetToken)\n",
    "        for x in streetTokenL:\n",
    "            if x in setRemoveKey:\n",
    "                streetTokenL.remove(x)\n",
    "        cleaned = ' '.join(streetTokenL)\n",
    "        askGoogleDic[street] = cleaned\n",
    "        wikiAskDic.append(cleaned) \n",
    "        \n",
    "save_file(cityKey+\"_askGoogle\",askGoogleDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Carico in wikiList via e url wiki \n",
    "with open('W_%s_askGoogle.json' %cityKey) as json_data:\n",
    "    wikiList = json.load(json_data) \n",
    "#print wikiList   \n",
    "#print name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contentWiki = addKeyFromWiki(name_id,wikiList)\n",
    "#print contentWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendo il file di Scrapy risultante da quello con wiki e creo un json con idx e dictionary con i value\n",
    "scrapyFile = open_file(\"%s_dedicationInfo.json\" %cityKey)\n",
    "print len(scrapyFile)\n",
    "contentWithScrapy = addScrapy(contentWiki,scrapyFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alternativa 1 = tengo solo le istanze di human o period occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducedGeo = dicReducedGeoOnlyInstance(geoJson,contentWithScrapy)\n",
    "reducedGeo.save(\"%s_final_Inst.geojson\" %cityKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alternativa 2 = al geoJson iniziale (che contiene tutte le vie) \n",
    "#scremo solo su vie esistenti di ContenWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reducedGeo = dicReducedGeo(geoJson,contentWithScrapy)\n",
    "reducedGeo.save(\"%s_final.geojson\" %cityKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FINE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
